{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBKrO0KnPf62",
        "outputId": "5dbef806-a228-4707-bbd0-9220f4499f28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.26.5.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (3.0.1)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.26.5-py3-none-any.whl size=67620 sha256=25b49211df0da15fec5d3ccc3a77917801b1a039fdb426bcf8c23f8a3b92bd05\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/47/99/8273a59fbd59c303e8ff175416d5c1c9c03a2e83ebf7525a99\n",
            "Successfully built openai\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.26.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Kylers amazing ML Idea: \n",
        "# Use a generative pre trained model. \n",
        "# I'm curious if we fed it batch csv data per vendor and then made it generate summaries \n",
        "# Limit the summaries to under 250 words. \n",
        "# Start checking the data and see if it works\n",
        "# if it does, throw it on the checkbook. \n",
        "# \"powered by AI\" take that socrata. \n",
        "# Developed by Vartan A. \n",
        "# See ReadMe On the repo for code explantion \n",
        "# --------------------------------------------------\n",
        "\n",
        "\n",
        "#First things first, we have to set up the OpenAI API key and model ID. Decided to use gpt3 because its powerful and awesome. \n",
        "openai.api_key = \"OPENAI_API_KEY\"\n",
        "model_engine = \"text-davinci-002\"\n",
        "\n",
        "# Second thing, we need to define a function to generate summaries using GPT-3 Pre-trained model. \n",
        "def generate_summaries(batches):\n",
        "    prompt = \"Please summarize the following text into 250 words or less:\\n\"\n",
        "    prompt += \"\\n\".join([\"\\n\".join(batch) + \"\\n\" for batch in batches]) + \"\\nSummary:\"\n",
        "    responses = openai.Completion.create(\n",
        "        engine=model_engine,\n",
        "        prompt=prompt,\n",
        "        max_tokens=512,\n",
        "        n=len(batches),\n",
        "        stop=None,\n",
        "        temperature=0.5,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        logprobs=0\n",
        "    )\n",
        "    summaries = []\n",
        "    for i, choice in enumerate(responses.choices):\n",
        "        start = sum([len(batch) for batch in batches[:i]])\n",
        "        end = start + len(batches[i])\n",
        "        summaries += [choice.text.strip()] * len(batches[i])\n",
        "    return summaries\n",
        "\n",
        "# Third, read the CSV data and generate summaries using GPT-3. \n",
        "for vendor in ['/content/vendor1.csv']:\n",
        "    data = pd.read_csv(vendor)\n",
        "    data = data.astype(str)\n",
        "\n",
        "    # Group data by account, fund, vendor, description, item, and department columns\n",
        "    # TODO: Inside the struct list just add Vendor for the vendors name from the column list if a csv has it.\n",
        "    grouped_data = data.groupby(['Account', 'Fund', 'Desc', 'Item', 'Dept'])\n",
        "\n",
        "    # Next, summarize data for each group\n",
        "    summaries = []\n",
        "    batch_size = 5\n",
        "    for group, group_data in grouped_data:\n",
        "        batch = group_data['Item'].tolist()\n",
        "        summaries += generate_summaries([batch])\n",
        "\n",
        "    # At last, write the summaries to a new CSV file\n",
        "    data['summary'] = summaries\n",
        "    data.to_csv(vendor[:-4]+'summarize.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "ytWHqxqDYz76"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}